{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SampleRNN pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiisuri/hello-world/blob/master/SampleRNN_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9qAoTRPsUIe"
      },
      "source": [
        "%%bash\n",
        "apt install bc\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2oBlYXvxiUZ"
      },
      "source": [
        "%%bash\n",
        "conda install --channel defaults conda python=3.6 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKyqMt40yucq"
      },
      "source": [
        "!conda --version\n",
        "!python --version # now returns Python 3.6.10 :: Anaconda, Inc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqfXVGpUwuky"
      },
      "source": [
        "!conda install -y pytorch=0.4.1 cuda92 -c pytorch\n",
        "!pip install -q librosa==0.5.1 matplotlib==2.1.0 natsort==5.1.0 ffmpeg ffprobe youtube-dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhJRBkWLWENk"
      },
      "source": [
        "!git clone https://github.com/deepsound-project/samplernn-pytorch.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RPQ6cgyOsbC"
      },
      "source": [
        "cd /content/samplernn-pytorch/datasets/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMxOZbjcVJMZ"
      },
      "source": [
        "# !./download-from-youtube.sh \"https://www.youtube.com/watch?v=k_Ibvyi40fs\" 8 jazzpiano\n",
        "!./download-from-youtube.sh \"https://www.youtube.com/watch?v=9gv8UeQ_M4M\" 2 voices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU1jKarG1Quv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ab8266b-cee9-4b3f-a2cd-7797239c2259"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/samplernn-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOrFPF0IXbPL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d222c14b-6e83-4aef-9e88-9c4357229183"
      },
      "source": [
        "!python train.py --exp VOICES --frame_sizes 16 4 --n_rnn 3 --dataset voices --batch_size 64"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/samplernn-pytorch/model.py:60: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
            "  init.kaiming_uniform(self.input_expand.weight)\n",
            "/content/samplernn-pytorch/model.py:61: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  init.constant(self.input_expand.bias, 0)\n",
            "/content/samplernn-pytorch/nn.py:48: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))\n",
            "/content/samplernn-pytorch/model.py:76: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  init.constant(getattr(self.rnn, 'bias_ih_l{}'.format(i)), 0)\n",
            "/content/samplernn-pytorch/nn.py:62: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  init(chunk)\n",
            "/content/samplernn-pytorch/model.py:82: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  init.constant(getattr(self.rnn, 'bias_hh_l{}'.format(i)), 0)\n",
            "/content/samplernn-pytorch/nn.py:31: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  nn.init.constant(self.bias, 0)\n",
            "/content/samplernn-pytorch/model.py:90: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  self.upsampling.conv_t.weight, -np.sqrt(6 / dim), np.sqrt(6 / dim)\n",
            "/content/samplernn-pytorch/model.py:92: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  init.constant(self.upsampling.bias, 0)\n",
            "/content/samplernn-pytorch/model.py:141: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
            "  init.kaiming_uniform(self.input.weight)\n",
            "/content/samplernn-pytorch/model.py:150: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
            "  init.kaiming_uniform(self.hidden.weight)\n",
            "/content/samplernn-pytorch/model.py:151: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  init.constant(self.hidden.bias, 0)\n",
            "/content/samplernn-pytorch/model.py:161: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  init.constant(self.output.bias, 0)\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 8.8433  (0.0884)\ttime: 0s\n",
            "training_loss: 6.3077  (0.1506)\ttime: 0s\n",
            "training_loss: 6.1486  (0.2106)\ttime: 1s\n",
            "training_loss: 5.9268  (0.2678)\ttime: 1s\n",
            "training_loss: 5.4967  (0.3201)\ttime: 2s\n",
            "training_loss: 5.4405  (0.3713)\ttime: 2s\n",
            "training_loss: 5.4482  (0.4220)\ttime: 2s\n",
            "training_loss: 5.4101  (0.4719)\ttime: 3s\n",
            "training_loss: 5.1468  (0.5187)\ttime: 3s\n",
            "training_loss: 5.1926  (0.5654)\ttime: 3s\n",
            "training_loss: 5.0823  (0.6106)\ttime: 4s\n",
            "training_loss: 5.1461  (0.6559)\ttime: 4s\n",
            "training_loss: 5.2543  (0.7019)\ttime: 5s\n",
            "training_loss: 4.8384  (0.7433)\ttime: 5s\n",
            "training_loss: 4.8790  (0.7846)\ttime: 5s\n",
            "training_loss: 4.6524  (0.8233)\ttime: 6s\n",
            "training_loss: 4.6718  (0.8618)\ttime: 6s\n",
            "training_loss: 4.9065  (0.9022)\ttime: 7s\n",
            "training_loss: 4.4317  (0.9375)\ttime: 7s\n",
            "training_loss: 4.2820  (0.9710)\ttime: 7s\n",
            "training_loss: 4.5535  (1.0068)\ttime: 8s\n",
            "training_loss: 4.5441  (1.0422)\ttime: 8s\n",
            "training_loss: 4.4399  (1.0762)\ttime: 9s\n",
            "training_loss: 4.3093  (1.1085)\ttime: 9s\n",
            "training_loss: 4.0943  (1.1383)\ttime: 9s\n",
            "training_loss: 3.9182  (1.1661)\ttime: 10s\n",
            "training_loss: 3.8554  (1.1930)\ttime: 10s\n",
            "training_loss: 4.3135  (1.2242)\ttime: 10s\n",
            "training_loss: 4.8221  (1.2602)\ttime: 11s\n",
            "training_loss: 4.6284  (1.2939)\ttime: 11s\n",
            "training_loss: 4.6129  (1.3271)\ttime: 12s\n",
            "training_loss: 4.7296  (1.3611)\ttime: 12s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 4.1704  (1.3892)\ttime: 26s\n",
            "training_loss: 4.1580  (1.4169)\ttime: 27s\n",
            "training_loss: 4.1861  (1.4446)\ttime: 27s\n",
            "training_loss: 4.4534  (1.4747)\ttime: 27s\n",
            "training_loss: 4.5446  (1.5054)\ttime: 28s\n",
            "training_loss: 4.2742  (1.5331)\ttime: 28s\n",
            "training_loss: 4.0924  (1.5587)\ttime: 29s\n",
            "training_loss: 3.9879  (1.5830)\ttime: 29s\n",
            "training_loss: 3.8065  (1.6052)\ttime: 29s\n",
            "training_loss: 3.9448  (1.6286)\ttime: 30s\n",
            "training_loss: 3.9370  (1.6517)\ttime: 30s\n",
            "training_loss: 4.0913  (1.6761)\ttime: 31s\n",
            "training_loss: 4.0990  (1.7003)\ttime: 31s\n",
            "training_loss: 3.9826  (1.7231)\ttime: 31s\n",
            "training_loss: 4.2306  (1.7482)\ttime: 32s\n",
            "training_loss: 4.4439  (1.7751)\ttime: 32s\n",
            "training_loss: 4.4012  (1.8014)\ttime: 32s\n",
            "training_loss: 4.0726  (1.8241)\ttime: 33s\n",
            "training_loss: 3.7611  (1.8435)\ttime: 33s\n",
            "training_loss: 3.7303  (1.8624)\ttime: 34s\n",
            "training_loss: 3.9556  (1.8833)\ttime: 34s\n",
            "training_loss: 3.9038  (1.9035)\ttime: 34s\n",
            "training_loss: 4.0902  (1.9254)\ttime: 35s\n",
            "training_loss: 4.0581  (1.9467)\ttime: 35s\n",
            "training_loss: 3.7028  (1.9643)\ttime: 36s\n",
            "training_loss: 3.7909  (1.9825)\ttime: 36s\n",
            "training_loss: 4.0454  (2.0031)\ttime: 36s\n",
            "training_loss: 4.1285  (2.0244)\ttime: 37s\n",
            "training_loss: 3.8942  (2.0431)\ttime: 37s\n",
            "training_loss: 3.7861  (2.0605)\ttime: 37s\n",
            "training_loss: 3.9059  (2.0790)\ttime: 38s\n",
            "training_loss: 3.8341  (2.0965)\ttime: 38s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 4.1435  (2.1170)\ttime: 52s\n",
            "training_loss: 4.0251  (2.1361)\ttime: 53s\n",
            "training_loss: 4.0071  (2.1548)\ttime: 53s\n",
            "training_loss: 3.7497  (2.1707)\ttime: 54s\n",
            "training_loss: 3.8979  (2.1880)\ttime: 54s\n",
            "training_loss: 3.9666  (2.2058)\ttime: 54s\n",
            "training_loss: 3.7946  (2.2217)\ttime: 55s\n",
            "training_loss: 3.6937  (2.2364)\ttime: 55s\n",
            "training_loss: 3.5263  (2.2493)\ttime: 55s\n",
            "training_loss: 3.3463  (2.2603)\ttime: 56s\n",
            "training_loss: 3.3104  (2.2708)\ttime: 56s\n",
            "training_loss: 3.3743  (2.2818)\ttime: 57s\n",
            "training_loss: 3.0008  (2.2890)\ttime: 57s\n",
            "training_loss: 3.2422  (2.2985)\ttime: 57s\n",
            "training_loss: 3.3755  (2.3093)\ttime: 58s\n",
            "training_loss: 3.4524  (2.3207)\ttime: 58s\n",
            "training_loss: 3.5132  (2.3327)\ttime: 59s\n",
            "training_loss: 3.5652  (2.3450)\ttime: 59s\n",
            "training_loss: 3.3546  (2.3551)\ttime: 59s\n",
            "training_loss: 3.3898  (2.3654)\ttime: 60s\n",
            "training_loss: 3.3653  (2.3754)\ttime: 60s\n",
            "training_loss: 3.4232  (2.3859)\ttime: 61s\n",
            "training_loss: 3.3473  (2.3955)\ttime: 61s\n",
            "training_loss: 3.5415  (2.4070)\ttime: 61s\n",
            "training_loss: 3.4647  (2.4176)\ttime: 62s\n",
            "training_loss: 3.2350  (2.4257)\ttime: 62s\n",
            "training_loss: 3.4240  (2.4357)\ttime: 62s\n",
            "training_loss: 3.4868  (2.4462)\ttime: 63s\n",
            "training_loss: 3.2877  (2.4546)\ttime: 63s\n",
            "training_loss: 3.2912  (2.4630)\ttime: 64s\n",
            "training_loss: 3.4873  (2.4732)\ttime: 64s\n",
            "training_loss: 3.3767  (2.4823)\ttime: 64s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 3.6859  (2.4943)\ttime: 79s\n",
            "training_loss: 3.6176  (2.5056)\ttime: 79s\n",
            "training_loss: 3.7340  (2.5178)\ttime: 79s\n",
            "training_loss: 3.5649  (2.5283)\ttime: 80s\n",
            "training_loss: 3.6006  (2.5390)\ttime: 80s\n",
            "training_loss: 3.4200  (2.5478)\ttime: 80s\n",
            "training_loss: 3.5841  (2.5582)\ttime: 81s\n",
            "training_loss: 3.4021  (2.5666)\ttime: 81s\n",
            "training_loss: 3.3580  (2.5746)\ttime: 82s\n",
            "training_loss: 3.2986  (2.5818)\ttime: 82s\n",
            "training_loss: 3.1536  (2.5875)\ttime: 82s\n",
            "training_loss: 3.2620  (2.5943)\ttime: 83s\n",
            "training_loss: 3.3669  (2.6020)\ttime: 83s\n",
            "training_loss: 3.6450  (2.6124)\ttime: 84s\n",
            "training_loss: 3.6658  (2.6229)\ttime: 84s\n",
            "training_loss: 3.4658  (2.6314)\ttime: 84s\n",
            "training_loss: 3.3623  (2.6387)\ttime: 85s\n",
            "training_loss: 3.5029  (2.6473)\ttime: 85s\n",
            "training_loss: 3.5034  (2.6559)\ttime: 85s\n",
            "training_loss: 3.2093  (2.6614)\ttime: 86s\n",
            "training_loss: 3.4411  (2.6692)\ttime: 86s\n",
            "training_loss: 3.2467  (2.6750)\ttime: 87s\n",
            "training_loss: 3.0694  (2.6789)\ttime: 87s\n",
            "training_loss: 3.3266  (2.6854)\ttime: 87s\n",
            "training_loss: 3.5783  (2.6943)\ttime: 88s\n",
            "training_loss: 3.5088  (2.7025)\ttime: 88s\n",
            "training_loss: 3.7429  (2.7129)\ttime: 89s\n",
            "training_loss: 3.5556  (2.7213)\ttime: 89s\n",
            "training_loss: 3.5522  (2.7296)\ttime: 89s\n",
            "training_loss: 3.8248  (2.7406)\ttime: 90s\n",
            "training_loss: 3.9104  (2.7523)\ttime: 90s\n",
            "training_loss: 3.6487  (2.7612)\ttime: 90s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 3.5533  (2.7692)\ttime: 105s\n",
            "training_loss: 3.4463  (2.7759)\ttime: 105s\n",
            "training_loss: 3.2214  (2.7804)\ttime: 106s\n",
            "training_loss: 3.5338  (2.7879)\ttime: 106s\n",
            "training_loss: 3.4843  (2.7949)\ttime: 106s\n",
            "training_loss: 3.3881  (2.8008)\ttime: 107s\n",
            "training_loss: 3.4062  (2.8069)\ttime: 107s\n",
            "training_loss: 3.4073  (2.8129)\ttime: 107s\n",
            "training_loss: 3.7265  (2.8220)\ttime: 108s\n",
            "training_loss: 3.6129  (2.8299)\ttime: 108s\n",
            "training_loss: 3.5372  (2.8370)\ttime: 109s\n",
            "training_loss: 3.3110  (2.8417)\ttime: 109s\n",
            "training_loss: 3.1607  (2.8449)\ttime: 109s\n",
            "training_loss: 3.5854  (2.8523)\ttime: 110s\n",
            "training_loss: 3.6811  (2.8606)\ttime: 110s\n",
            "training_loss: 3.5455  (2.8675)\ttime: 111s\n",
            "training_loss: 3.2793  (2.8716)\ttime: 111s\n",
            "training_loss: 3.2933  (2.8758)\ttime: 111s\n",
            "training_loss: 3.3338  (2.8804)\ttime: 112s\n",
            "training_loss: 3.3832  (2.8854)\ttime: 112s\n",
            "training_loss: 3.6356  (2.8929)\ttime: 113s\n",
            "training_loss: 3.4163  (2.8981)\ttime: 113s\n",
            "training_loss: 3.3775  (2.9029)\ttime: 113s\n",
            "training_loss: 3.2203  (2.9061)\ttime: 114s\n",
            "training_loss: 3.1457  (2.9085)\ttime: 114s\n",
            "training_loss: 3.3839  (2.9133)\ttime: 114s\n",
            "training_loss: 3.2035  (2.9162)\ttime: 115s\n",
            "training_loss: 3.0876  (2.9179)\ttime: 115s\n",
            "training_loss: 3.3138  (2.9218)\ttime: 116s\n",
            "training_loss: 3.3537  (2.9262)\ttime: 116s\n",
            "training_loss: 3.3223  (2.9301)\ttime: 116s\n",
            "training_loss: 3.2823  (2.9336)\ttime: 117s\n",
            "/content/samplernn-pytorch/trainer/plugins.py:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input = Variable(input, volatile=True)\n",
            "/content/samplernn-pytorch/trainer/plugins.py:66: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  batch_target = Variable(batch_target, volatile=True)\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "/content/samplernn-pytorch/trainer/plugins.py:72: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  .data[0] * batch_size\n",
            "/content/samplernn-pytorch/trainer/plugins.py:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input = Variable(input, volatile=True)\n",
            "/content/samplernn-pytorch/trainer/plugins.py:66: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  batch_target = Variable(batch_target, volatile=True)\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "/content/samplernn-pytorch/trainer/plugins.py:72: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  .data[0] * batch_size\n",
            "/content/samplernn-pytorch/model.py:267: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  volatile=True\n",
            "/content/samplernn-pytorch/model.py:287: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  volatile=True\n",
            "################################################################################\n",
            "Epoch summary:\n",
            "training_loss: 3.8918\tvalidation_loss: 3.5138\ttest_loss: 3.2818\n",
            "################################################################################\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 3.2716  (2.9370)\ttime: 215s\n",
            "training_loss: 3.3613  (2.9413)\ttime: 216s\n",
            "training_loss: 3.3732  (2.9456)\ttime: 216s\n",
            "training_loss: 3.3037  (2.9492)\ttime: 216s\n",
            "training_loss: 3.4295  (2.9540)\ttime: 217s\n",
            "training_loss: 3.2568  (2.9570)\ttime: 217s\n",
            "training_loss: 3.2585  (2.9600)\ttime: 218s\n",
            "training_loss: 3.2206  (2.9626)\ttime: 218s\n",
            "training_loss: 3.3635  (2.9666)\ttime: 218s\n",
            "training_loss: 3.4671  (2.9716)\ttime: 219s\n",
            "training_loss: 3.6078  (2.9780)\ttime: 219s\n",
            "training_loss: 3.2435  (2.9806)\ttime: 220s\n",
            "training_loss: 3.0601  (2.9814)\ttime: 220s\n",
            "training_loss: 3.3160  (2.9848)\ttime: 220s\n",
            "training_loss: 3.3409  (2.9883)\ttime: 221s\n",
            "training_loss: 3.5953  (2.9944)\ttime: 221s\n",
            "training_loss: 3.5052  (2.9995)\ttime: 221s\n",
            "training_loss: 3.2596  (3.0021)\ttime: 222s\n",
            "training_loss: 2.8521  (3.0006)\ttime: 222s\n",
            "training_loss: 2.6250  (2.9969)\ttime: 223s\n",
            "training_loss: 2.7433  (2.9943)\ttime: 223s\n",
            "training_loss: 3.0634  (2.9950)\ttime: 223s\n",
            "training_loss: 3.3076  (2.9982)\ttime: 224s\n",
            "training_loss: 3.2994  (3.0012)\ttime: 224s\n",
            "training_loss: 3.2695  (3.0038)\ttime: 225s\n",
            "training_loss: 3.1857  (3.0057)\ttime: 225s\n",
            "training_loss: 3.2598  (3.0082)\ttime: 225s\n",
            "training_loss: 3.1857  (3.0100)\ttime: 226s\n",
            "training_loss: 3.3995  (3.0139)\ttime: 226s\n",
            "training_loss: 3.3447  (3.0172)\ttime: 227s\n",
            "training_loss: 3.2877  (3.0199)\ttime: 227s\n",
            "training_loss: 3.3688  (3.0234)\ttime: 227s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 3.0931  (3.0241)\ttime: 241s\n",
            "training_loss: 2.9363  (3.0232)\ttime: 242s\n",
            "training_loss: 3.2691  (3.0257)\ttime: 242s\n",
            "training_loss: 3.5771  (3.0312)\ttime: 242s\n",
            "training_loss: 3.2795  (3.0337)\ttime: 243s\n",
            "training_loss: 3.0803  (3.0341)\ttime: 243s\n",
            "training_loss: 3.0828  (3.0346)\ttime: 244s\n",
            "training_loss: 2.9688  (3.0340)\ttime: 244s\n",
            "training_loss: 2.9799  (3.0334)\ttime: 244s\n",
            "training_loss: 3.0735  (3.0338)\ttime: 245s\n",
            "training_loss: 3.1803  (3.0353)\ttime: 245s\n",
            "training_loss: 3.1515  (3.0364)\ttime: 246s\n",
            "training_loss: 3.1729  (3.0378)\ttime: 246s\n",
            "training_loss: 3.2846  (3.0403)\ttime: 246s\n",
            "training_loss: 3.6570  (3.0464)\ttime: 247s\n",
            "training_loss: 3.3828  (3.0498)\ttime: 247s\n",
            "training_loss: 3.1906  (3.0512)\ttime: 248s\n",
            "training_loss: 3.4259  (3.0550)\ttime: 248s\n",
            "training_loss: 3.4358  (3.0588)\ttime: 248s\n",
            "training_loss: 3.1228  (3.0594)\ttime: 249s\n",
            "training_loss: 3.0975  (3.0598)\ttime: 249s\n",
            "training_loss: 2.8692  (3.0579)\ttime: 249s\n",
            "training_loss: 2.9420  (3.0567)\ttime: 250s\n",
            "training_loss: 3.2905  (3.0591)\ttime: 250s\n",
            "training_loss: 3.2039  (3.0605)\ttime: 251s\n",
            "training_loss: 3.0673  (3.0606)\ttime: 251s\n",
            "training_loss: 3.1712  (3.0617)\ttime: 251s\n",
            "training_loss: 3.2377  (3.0634)\ttime: 252s\n",
            "training_loss: 3.3445  (3.0663)\ttime: 252s\n",
            "training_loss: 3.3296  (3.0689)\ttime: 253s\n",
            "training_loss: 3.4107  (3.0723)\ttime: 253s\n",
            "training_loss: 3.2018  (3.0736)\ttime: 253s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 3.0717  (3.0736)\ttime: 268s\n",
            "training_loss: 3.1932  (3.0748)\ttime: 268s\n",
            "training_loss: 3.1321  (3.0753)\ttime: 268s\n",
            "training_loss: 3.1298  (3.0759)\ttime: 269s\n",
            "training_loss: 3.1830  (3.0770)\ttime: 269s\n",
            "training_loss: 3.1736  (3.0779)\ttime: 269s\n",
            "training_loss: 3.2837  (3.0800)\ttime: 270s\n",
            "training_loss: 3.3252  (3.0824)\ttime: 270s\n",
            "training_loss: 3.2233  (3.0838)\ttime: 271s\n",
            "training_loss: 3.0319  (3.0833)\ttime: 271s\n",
            "training_loss: 2.8741  (3.0812)\ttime: 271s\n",
            "training_loss: 3.1789  (3.0822)\ttime: 272s\n",
            "training_loss: 3.1288  (3.0827)\ttime: 272s\n",
            "training_loss: 3.0315  (3.0822)\ttime: 273s\n",
            "training_loss: 3.0176  (3.0815)\ttime: 273s\n",
            "training_loss: 2.9239  (3.0799)\ttime: 273s\n",
            "training_loss: 2.9165  (3.0783)\ttime: 274s\n",
            "training_loss: 2.7843  (3.0754)\ttime: 274s\n",
            "training_loss: 2.7462  (3.0721)\ttime: 275s\n",
            "training_loss: 2.9450  (3.0708)\ttime: 275s\n",
            "training_loss: 3.1009  (3.0711)\ttime: 275s\n",
            "training_loss: 2.8844  (3.0692)\ttime: 276s\n",
            "training_loss: 2.6599  (3.0651)\ttime: 276s\n",
            "training_loss: 2.7989  (3.0625)\ttime: 276s\n",
            "training_loss: 2.8640  (3.0605)\ttime: 277s\n",
            "training_loss: 3.0557  (3.0605)\ttime: 277s\n",
            "training_loss: 3.0033  (3.0599)\ttime: 278s\n",
            "training_loss: 2.8708  (3.0580)\ttime: 278s\n",
            "training_loss: 2.7531  (3.0549)\ttime: 278s\n",
            "training_loss: 2.9975  (3.0544)\ttime: 279s\n",
            "training_loss: 3.2990  (3.0568)\ttime: 279s\n",
            "training_loss: 3.0636  (3.0569)\ttime: 279s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 3.4109  (3.0604)\ttime: 294s\n",
            "training_loss: 3.3180  (3.0630)\ttime: 294s\n",
            "training_loss: 3.3983  (3.0663)\ttime: 295s\n",
            "training_loss: 3.4822  (3.0705)\ttime: 295s\n",
            "training_loss: 3.3362  (3.0732)\ttime: 296s\n",
            "training_loss: 3.2988  (3.0754)\ttime: 296s\n",
            "training_loss: 3.3232  (3.0779)\ttime: 296s\n",
            "training_loss: 3.2031  (3.0792)\ttime: 297s\n",
            "training_loss: 2.9691  (3.0781)\ttime: 297s\n",
            "training_loss: 2.7220  (3.0745)\ttime: 297s\n",
            "training_loss: 2.6067  (3.0698)\ttime: 298s\n",
            "training_loss: 2.8060  (3.0672)\ttime: 298s\n",
            "training_loss: 2.8717  (3.0652)\ttime: 299s\n",
            "training_loss: 2.9785  (3.0644)\ttime: 299s\n",
            "training_loss: 2.8466  (3.0622)\ttime: 299s\n",
            "training_loss: 3.0434  (3.0620)\ttime: 300s\n",
            "training_loss: 3.1035  (3.0624)\ttime: 300s\n",
            "training_loss: 3.0768  (3.0625)\ttime: 301s\n",
            "training_loss: 2.9985  (3.0619)\ttime: 301s\n",
            "training_loss: 2.9784  (3.0611)\ttime: 301s\n",
            "training_loss: 3.1471  (3.0619)\ttime: 302s\n",
            "training_loss: 3.1324  (3.0626)\ttime: 302s\n",
            "training_loss: 3.0306  (3.0623)\ttime: 303s\n",
            "training_loss: 2.9322  (3.0610)\ttime: 303s\n",
            "training_loss: 2.8918  (3.0593)\ttime: 303s\n",
            "training_loss: 2.8399  (3.0571)\ttime: 304s\n",
            "training_loss: 2.7571  (3.0541)\ttime: 304s\n",
            "training_loss: 2.9194  (3.0528)\ttime: 305s\n",
            "training_loss: 3.0549  (3.0528)\ttime: 305s\n",
            "training_loss: 3.1477  (3.0538)\ttime: 305s\n",
            "training_loss: 3.2293  (3.0555)\ttime: 306s\n",
            "training_loss: 3.2147  (3.0571)\ttime: 306s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 3.2072  (3.0586)\ttime: 320s\n",
            "training_loss: 3.0285  (3.0583)\ttime: 320s\n",
            "training_loss: 3.0446  (3.0582)\ttime: 321s\n",
            "training_loss: 3.0289  (3.0579)\ttime: 321s\n",
            "training_loss: 3.1878  (3.0592)\ttime: 322s\n",
            "training_loss: 3.1981  (3.0606)\ttime: 322s\n",
            "training_loss: 3.2741  (3.0627)\ttime: 322s\n",
            "training_loss: 3.3058  (3.0651)\ttime: 323s\n",
            "training_loss: 3.3466  (3.0679)\ttime: 323s\n",
            "training_loss: 3.1263  (3.0685)\ttime: 324s\n",
            "training_loss: 2.9410  (3.0672)\ttime: 324s\n",
            "training_loss: 2.9811  (3.0664)\ttime: 324s\n",
            "training_loss: 3.0846  (3.0666)\ttime: 325s\n",
            "training_loss: 3.0175  (3.0661)\ttime: 325s\n",
            "training_loss: 3.2168  (3.0676)\ttime: 326s\n",
            "training_loss: 3.0122  (3.0670)\ttime: 326s\n",
            "training_loss: 2.9537  (3.0659)\ttime: 326s\n",
            "training_loss: 3.1752  (3.0670)\ttime: 327s\n",
            "training_loss: 3.0926  (3.0672)\ttime: 327s\n",
            "training_loss: 3.1281  (3.0679)\ttime: 327s\n",
            "training_loss: 3.5874  (3.0730)\ttime: 328s\n",
            "training_loss: 3.4876  (3.0772)\ttime: 328s\n",
            "training_loss: 3.2408  (3.0788)\ttime: 329s\n",
            "training_loss: 3.1790  (3.0798)\ttime: 329s\n",
            "training_loss: 3.0185  (3.0792)\ttime: 329s\n",
            "training_loss: 3.0158  (3.0786)\ttime: 330s\n",
            "training_loss: 3.1143  (3.0789)\ttime: 330s\n",
            "training_loss: 3.2036  (3.0802)\ttime: 331s\n",
            "training_loss: 3.1508  (3.0809)\ttime: 331s\n",
            "training_loss: 3.2125  (3.0822)\ttime: 331s\n",
            "training_loss: 3.1140  (3.0825)\ttime: 332s\n",
            "training_loss: 2.8720  (3.0804)\ttime: 332s\n",
            "/content/samplernn-pytorch/trainer/plugins.py:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input = Variable(input, volatile=True)\n",
            "/content/samplernn-pytorch/trainer/plugins.py:66: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  batch_target = Variable(batch_target, volatile=True)\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "/content/samplernn-pytorch/trainer/plugins.py:72: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  .data[0] * batch_size\n",
            "/content/samplernn-pytorch/trainer/plugins.py:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input = Variable(input, volatile=True)\n",
            "/content/samplernn-pytorch/trainer/plugins.py:66: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  batch_target = Variable(batch_target, volatile=True)\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "/content/samplernn-pytorch/trainer/plugins.py:72: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  .data[0] * batch_size\n",
            "/content/samplernn-pytorch/model.py:267: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  volatile=True\n",
            "/content/samplernn-pytorch/model.py:287: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  volatile=True\n",
            "################################################################################\n",
            "Epoch summary:\n",
            "training_loss: 3.1387\tvalidation_loss: 3.2205\ttest_loss: 3.0351\n",
            "################################################################################\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 3.0281  (3.0799)\ttime: 430s\n",
            "training_loss: 3.0993  (3.0801)\ttime: 430s\n",
            "training_loss: 3.2004  (3.0813)\ttime: 431s\n",
            "training_loss: 3.2445  (3.0829)\ttime: 431s\n",
            "training_loss: 3.1060  (3.0832)\ttime: 432s\n",
            "training_loss: 2.9715  (3.0820)\ttime: 432s\n",
            "training_loss: 2.9609  (3.0808)\ttime: 432s\n",
            "training_loss: 2.9218  (3.0792)\ttime: 433s\n",
            "training_loss: 2.7933  (3.0764)\ttime: 433s\n",
            "training_loss: 2.7775  (3.0734)\ttime: 434s\n",
            "training_loss: 2.8270  (3.0709)\ttime: 434s\n",
            "training_loss: 3.1402  (3.0716)\ttime: 434s\n",
            "training_loss: 2.8949  (3.0699)\ttime: 435s\n",
            "training_loss: 2.8661  (3.0678)\ttime: 435s\n",
            "training_loss: 3.1406  (3.0685)\ttime: 435s\n",
            "training_loss: 2.9637  (3.0675)\ttime: 436s\n",
            "training_loss: 2.7564  (3.0644)\ttime: 436s\n",
            "training_loss: 2.8151  (3.0619)\ttime: 437s\n",
            "training_loss: 2.6038  (3.0573)\ttime: 437s\n",
            "training_loss: 2.8312  (3.0551)\ttime: 437s\n",
            "training_loss: 3.0857  (3.0554)\ttime: 438s\n",
            "training_loss: 3.1450  (3.0563)\ttime: 438s\n",
            "training_loss: 3.2684  (3.0584)\ttime: 439s\n",
            "training_loss: 3.1660  (3.0595)\ttime: 439s\n",
            "training_loss: 2.9723  (3.0586)\ttime: 439s\n",
            "training_loss: 3.0324  (3.0583)\ttime: 440s\n",
            "training_loss: 3.1001  (3.0587)\ttime: 440s\n",
            "training_loss: 2.9929  (3.0581)\ttime: 441s\n",
            "training_loss: 3.0327  (3.0578)\ttime: 441s\n",
            "training_loss: 3.0668  (3.0579)\ttime: 441s\n",
            "training_loss: 3.2720  (3.0601)\ttime: 442s\n",
            "training_loss: 3.1791  (3.0612)\ttime: 442s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 3.2505  (3.0631)\ttime: 456s\n",
            "training_loss: 3.1117  (3.0636)\ttime: 456s\n",
            "training_loss: 3.0378  (3.0634)\ttime: 457s\n",
            "training_loss: 2.9810  (3.0625)\ttime: 457s\n",
            "training_loss: 3.0961  (3.0629)\ttime: 458s\n",
            "training_loss: 2.9521  (3.0618)\ttime: 458s\n",
            "training_loss: 2.9813  (3.0610)\ttime: 458s\n",
            "training_loss: 2.9420  (3.0598)\ttime: 459s\n",
            "training_loss: 2.8713  (3.0579)\ttime: 459s\n",
            "training_loss: 2.8856  (3.0562)\ttime: 459s\n",
            "training_loss: 2.9328  (3.0549)\ttime: 460s\n",
            "training_loss: 2.8500  (3.0529)\ttime: 460s\n",
            "training_loss: 2.8016  (3.0504)\ttime: 461s\n",
            "training_loss: 2.7400  (3.0473)\ttime: 461s\n",
            "training_loss: 2.7953  (3.0447)\ttime: 461s\n",
            "training_loss: 2.9142  (3.0434)\ttime: 462s\n",
            "training_loss: 3.0958  (3.0440)\ttime: 462s\n",
            "training_loss: 3.0684  (3.0442)\ttime: 463s\n",
            "training_loss: 2.8411  (3.0422)\ttime: 463s\n",
            "training_loss: 2.7220  (3.0390)\ttime: 463s\n",
            "training_loss: 2.8725  (3.0373)\ttime: 464s\n",
            "training_loss: 2.8652  (3.0356)\ttime: 464s\n",
            "training_loss: 2.8998  (3.0342)\ttime: 465s\n",
            "training_loss: 2.8143  (3.0320)\ttime: 465s\n",
            "training_loss: 2.6790  (3.0285)\ttime: 465s\n",
            "training_loss: 2.6686  (3.0249)\ttime: 466s\n",
            "training_loss: 2.8772  (3.0234)\ttime: 466s\n",
            "training_loss: 2.9632  (3.0228)\ttime: 466s\n",
            "training_loss: 2.8725  (3.0213)\ttime: 467s\n",
            "training_loss: 2.9571  (3.0207)\ttime: 467s\n",
            "training_loss: 3.1151  (3.0216)\ttime: 468s\n",
            "training_loss: 2.8235  (3.0196)\ttime: 468s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 3.1194  (3.0206)\ttime: 482s\n",
            "training_loss: 2.8044  (3.0185)\ttime: 482s\n",
            "training_loss: 2.7522  (3.0158)\ttime: 483s\n",
            "training_loss: 2.9554  (3.0152)\ttime: 483s\n",
            "training_loss: 3.2534  (3.0176)\ttime: 484s\n",
            "training_loss: 3.4064  (3.0215)\ttime: 484s\n",
            "training_loss: 3.3118  (3.0244)\ttime: 484s\n",
            "training_loss: 3.3079  (3.0272)\ttime: 485s\n",
            "training_loss: 3.2887  (3.0298)\ttime: 485s\n",
            "training_loss: 3.1929  (3.0315)\ttime: 486s\n",
            "training_loss: 3.0159  (3.0313)\ttime: 486s\n",
            "training_loss: 2.7544  (3.0285)\ttime: 486s\n",
            "training_loss: 2.7867  (3.0261)\ttime: 487s\n",
            "training_loss: 3.1096  (3.0270)\ttime: 487s\n",
            "training_loss: 3.0798  (3.0275)\ttime: 487s\n",
            "training_loss: 3.1098  (3.0283)\ttime: 488s\n",
            "training_loss: 3.0795  (3.0288)\ttime: 488s\n",
            "training_loss: 2.8953  (3.0275)\ttime: 489s\n",
            "training_loss: 2.7320  (3.0245)\ttime: 489s\n",
            "training_loss: 2.5099  (3.0194)\ttime: 489s\n",
            "training_loss: 2.5399  (3.0146)\ttime: 490s\n",
            "training_loss: 2.5319  (3.0098)\ttime: 490s\n",
            "training_loss: 2.4748  (3.0044)\ttime: 491s\n",
            "training_loss: 2.8048  (3.0024)\ttime: 491s\n",
            "training_loss: 2.8319  (3.0007)\ttime: 491s\n",
            "training_loss: 2.7593  (2.9983)\ttime: 492s\n",
            "training_loss: 2.7060  (2.9954)\ttime: 492s\n",
            "training_loss: 2.7591  (2.9930)\ttime: 493s\n",
            "training_loss: 2.7940  (2.9910)\ttime: 493s\n",
            "training_loss: 2.9160  (2.9903)\ttime: 493s\n",
            "training_loss: 2.8085  (2.9885)\ttime: 494s\n",
            "training_loss: 2.9193  (2.9878)\ttime: 494s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.9268  (2.9872)\ttime: 508s\n",
            "training_loss: 2.8977  (2.9863)\ttime: 509s\n",
            "training_loss: 2.8801  (2.9852)\ttime: 509s\n",
            "training_loss: 2.8507  (2.9839)\ttime: 509s\n",
            "training_loss: 2.7858  (2.9819)\ttime: 510s\n",
            "training_loss: 2.7139  (2.9792)\ttime: 510s\n",
            "training_loss: 2.7905  (2.9773)\ttime: 511s\n",
            "training_loss: 2.6538  (2.9741)\ttime: 511s\n",
            "training_loss: 2.6104  (2.9704)\ttime: 511s\n",
            "training_loss: 2.5688  (2.9664)\ttime: 512s\n",
            "training_loss: 2.3715  (2.9605)\ttime: 512s\n",
            "training_loss: 2.5427  (2.9563)\ttime: 513s\n",
            "training_loss: 2.5755  (2.9525)\ttime: 513s\n",
            "training_loss: 2.8297  (2.9513)\ttime: 513s\n",
            "training_loss: 2.9328  (2.9511)\ttime: 514s\n",
            "training_loss: 2.8329  (2.9499)\ttime: 514s\n",
            "training_loss: 2.8407  (2.9488)\ttime: 514s\n",
            "training_loss: 2.9465  (2.9488)\ttime: 515s\n",
            "training_loss: 2.8463  (2.9477)\ttime: 515s\n",
            "training_loss: 2.7347  (2.9456)\ttime: 516s\n",
            "training_loss: 2.9145  (2.9453)\ttime: 516s\n",
            "training_loss: 2.8067  (2.9439)\ttime: 516s\n",
            "training_loss: 2.9277  (2.9438)\ttime: 517s\n",
            "training_loss: 2.9570  (2.9439)\ttime: 517s\n",
            "training_loss: 2.7724  (2.9422)\ttime: 518s\n",
            "training_loss: 2.8087  (2.9408)\ttime: 518s\n",
            "training_loss: 2.9426  (2.9409)\ttime: 518s\n",
            "training_loss: 3.0064  (2.9415)\ttime: 519s\n",
            "training_loss: 3.1300  (2.9434)\ttime: 519s\n",
            "training_loss: 2.9768  (2.9437)\ttime: 520s\n",
            "training_loss: 2.8506  (2.9428)\ttime: 520s\n",
            "training_loss: 2.6162  (2.9395)\ttime: 520s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.9609  (2.9397)\ttime: 534s\n",
            "training_loss: 3.2048  (2.9424)\ttime: 535s\n",
            "training_loss: 3.3014  (2.9460)\ttime: 535s\n",
            "training_loss: 3.2852  (2.9494)\ttime: 536s\n",
            "training_loss: 2.9969  (2.9499)\ttime: 536s\n",
            "training_loss: 2.8813  (2.9492)\ttime: 536s\n",
            "training_loss: 2.9978  (2.9497)\ttime: 537s\n",
            "training_loss: 3.0570  (2.9507)\ttime: 537s\n",
            "training_loss: 3.1221  (2.9524)\ttime: 537s\n",
            "training_loss: 2.8363  (2.9513)\ttime: 538s\n",
            "training_loss: 2.9179  (2.9509)\ttime: 538s\n",
            "training_loss: 2.9777  (2.9512)\ttime: 539s\n",
            "training_loss: 3.0508  (2.9522)\ttime: 539s\n",
            "training_loss: 3.0034  (2.9527)\ttime: 539s\n",
            "training_loss: 3.0634  (2.9538)\ttime: 540s\n",
            "training_loss: 2.9709  (2.9540)\ttime: 540s\n",
            "training_loss: 2.9348  (2.9538)\ttime: 541s\n",
            "training_loss: 3.0987  (2.9553)\ttime: 541s\n",
            "training_loss: 3.0910  (2.9566)\ttime: 541s\n",
            "training_loss: 2.9299  (2.9563)\ttime: 542s\n",
            "training_loss: 3.0540  (2.9573)\ttime: 542s\n",
            "training_loss: 3.0418  (2.9582)\ttime: 543s\n",
            "training_loss: 2.7775  (2.9564)\ttime: 543s\n",
            "training_loss: 2.8498  (2.9553)\ttime: 543s\n",
            "training_loss: 2.9332  (2.9551)\ttime: 544s\n",
            "training_loss: 2.9265  (2.9548)\ttime: 544s\n",
            "training_loss: 2.9383  (2.9546)\ttime: 544s\n",
            "training_loss: 2.9840  (2.9549)\ttime: 545s\n",
            "training_loss: 3.0711  (2.9561)\ttime: 545s\n",
            "training_loss: 2.9629  (2.9561)\ttime: 546s\n",
            "training_loss: 2.9860  (2.9564)\ttime: 546s\n",
            "training_loss: 2.9292  (2.9562)\ttime: 546s\n",
            "/content/samplernn-pytorch/trainer/plugins.py:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input = Variable(input, volatile=True)\n",
            "/content/samplernn-pytorch/trainer/plugins.py:66: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  batch_target = Variable(batch_target, volatile=True)\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "/content/samplernn-pytorch/trainer/plugins.py:72: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  .data[0] * batch_size\n",
            "/content/samplernn-pytorch/trainer/plugins.py:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input = Variable(input, volatile=True)\n",
            "/content/samplernn-pytorch/trainer/plugins.py:66: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  batch_target = Variable(batch_target, volatile=True)\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "/content/samplernn-pytorch/trainer/plugins.py:72: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  .data[0] * batch_size\n",
            "/content/samplernn-pytorch/model.py:267: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  volatile=True\n",
            "/content/samplernn-pytorch/model.py:287: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  volatile=True\n",
            "################################################################################\n",
            "Epoch summary:\n",
            "training_loss: 2.9301\tvalidation_loss: 3.0970\ttest_loss: 2.9261\n",
            "################################################################################\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.9217  (2.9558)\ttime: 645s\n",
            "training_loss: 2.8972  (2.9552)\ttime: 645s\n",
            "training_loss: 2.7818  (2.9535)\ttime: 645s\n",
            "training_loss: 2.9665  (2.9536)\ttime: 646s\n",
            "training_loss: 2.9156  (2.9533)\ttime: 646s\n",
            "training_loss: 2.8715  (2.9524)\ttime: 647s\n",
            "training_loss: 3.1749  (2.9547)\ttime: 647s\n",
            "training_loss: 3.3546  (2.9587)\ttime: 647s\n",
            "training_loss: 3.0749  (2.9598)\ttime: 648s\n",
            "training_loss: 2.9334  (2.9596)\ttime: 648s\n",
            "training_loss: 2.8479  (2.9584)\ttime: 649s\n",
            "training_loss: 3.0148  (2.9590)\ttime: 649s\n",
            "training_loss: 2.8842  (2.9583)\ttime: 649s\n",
            "training_loss: 3.1216  (2.9599)\ttime: 650s\n",
            "training_loss: 3.2012  (2.9623)\ttime: 650s\n",
            "training_loss: 3.1818  (2.9645)\ttime: 650s\n",
            "training_loss: 3.1280  (2.9661)\ttime: 651s\n",
            "training_loss: 2.8866  (2.9653)\ttime: 651s\n",
            "training_loss: 2.6600  (2.9623)\ttime: 652s\n",
            "training_loss: 2.5748  (2.9584)\ttime: 652s\n",
            "training_loss: 2.5686  (2.9545)\ttime: 652s\n",
            "training_loss: 2.5318  (2.9503)\ttime: 653s\n",
            "training_loss: 2.6460  (2.9472)\ttime: 653s\n",
            "training_loss: 2.7656  (2.9454)\ttime: 654s\n",
            "training_loss: 2.6987  (2.9430)\ttime: 654s\n",
            "training_loss: 2.7525  (2.9411)\ttime: 654s\n",
            "training_loss: 2.8862  (2.9405)\ttime: 655s\n",
            "training_loss: 2.9861  (2.9410)\ttime: 655s\n",
            "training_loss: 3.0703  (2.9423)\ttime: 656s\n",
            "training_loss: 3.0551  (2.9434)\ttime: 656s\n",
            "training_loss: 2.9485  (2.9434)\ttime: 656s\n",
            "training_loss: 2.7930  (2.9419)\ttime: 656s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.9931  (2.9424)\ttime: 671s\n",
            "training_loss: 3.1496  (2.9445)\ttime: 671s\n",
            "training_loss: 3.1333  (2.9464)\ttime: 672s\n",
            "training_loss: 3.1598  (2.9485)\ttime: 672s\n",
            "training_loss: 3.0445  (2.9495)\ttime: 672s\n",
            "training_loss: 2.9590  (2.9496)\ttime: 673s\n",
            "training_loss: 3.1122  (2.9512)\ttime: 673s\n",
            "training_loss: 2.9277  (2.9510)\ttime: 674s\n",
            "training_loss: 2.9260  (2.9507)\ttime: 674s\n",
            "training_loss: 2.9101  (2.9503)\ttime: 674s\n",
            "training_loss: 3.0364  (2.9512)\ttime: 675s\n",
            "training_loss: 2.8899  (2.9506)\ttime: 675s\n",
            "training_loss: 2.8903  (2.9500)\ttime: 676s\n",
            "training_loss: 3.0857  (2.9513)\ttime: 676s\n",
            "training_loss: 3.1849  (2.9537)\ttime: 676s\n",
            "training_loss: 3.0579  (2.9547)\ttime: 677s\n",
            "training_loss: 2.9988  (2.9551)\ttime: 677s\n",
            "training_loss: 2.8497  (2.9541)\ttime: 677s\n",
            "training_loss: 2.7206  (2.9518)\ttime: 678s\n",
            "training_loss: 2.8236  (2.9505)\ttime: 678s\n",
            "training_loss: 3.0657  (2.9516)\ttime: 679s\n",
            "training_loss: 2.7011  (2.9491)\ttime: 679s\n",
            "training_loss: 2.6601  (2.9462)\ttime: 679s\n",
            "training_loss: 2.6396  (2.9432)\ttime: 680s\n",
            "training_loss: 2.7520  (2.9413)\ttime: 680s\n",
            "training_loss: 2.6526  (2.9384)\ttime: 681s\n",
            "training_loss: 2.6784  (2.9358)\ttime: 681s\n",
            "training_loss: 2.6799  (2.9332)\ttime: 681s\n",
            "training_loss: 2.7054  (2.9309)\ttime: 682s\n",
            "training_loss: 2.8783  (2.9304)\ttime: 682s\n",
            "training_loss: 3.0173  (2.9313)\ttime: 683s\n",
            "training_loss: 2.8954  (2.9309)\ttime: 683s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.6314  (2.9279)\ttime: 697s\n",
            "training_loss: 2.5757  (2.9244)\ttime: 697s\n",
            "training_loss: 2.6182  (2.9213)\ttime: 698s\n",
            "training_loss: 2.6365  (2.9185)\ttime: 698s\n",
            "training_loss: 2.9062  (2.9184)\ttime: 699s\n",
            "training_loss: 2.8487  (2.9177)\ttime: 699s\n",
            "training_loss: 2.6340  (2.9148)\ttime: 699s\n",
            "training_loss: 2.6305  (2.9120)\ttime: 700s\n",
            "training_loss: 2.6997  (2.9099)\ttime: 700s\n",
            "training_loss: 2.8374  (2.9091)\ttime: 701s\n",
            "training_loss: 2.7744  (2.9078)\ttime: 701s\n",
            "training_loss: 2.6290  (2.9050)\ttime: 701s\n",
            "training_loss: 2.5052  (2.9010)\ttime: 702s\n",
            "training_loss: 2.7173  (2.8992)\ttime: 702s\n",
            "training_loss: 2.8771  (2.8990)\ttime: 702s\n",
            "training_loss: 2.8466  (2.8984)\ttime: 703s\n",
            "training_loss: 2.7015  (2.8965)\ttime: 703s\n",
            "training_loss: 2.7619  (2.8951)\ttime: 704s\n",
            "training_loss: 2.6449  (2.8926)\ttime: 704s\n",
            "training_loss: 2.7633  (2.8913)\ttime: 704s\n",
            "training_loss: 3.1440  (2.8938)\ttime: 705s\n",
            "training_loss: 3.1917  (2.8968)\ttime: 705s\n",
            "training_loss: 2.9424  (2.8973)\ttime: 706s\n",
            "training_loss: 2.8923  (2.8972)\ttime: 706s\n",
            "training_loss: 2.7921  (2.8962)\ttime: 706s\n",
            "training_loss: 2.8055  (2.8953)\ttime: 707s\n",
            "training_loss: 2.7938  (2.8943)\ttime: 707s\n",
            "training_loss: 2.7550  (2.8929)\ttime: 708s\n",
            "training_loss: 2.7772  (2.8917)\ttime: 708s\n",
            "training_loss: 2.8473  (2.8913)\ttime: 708s\n",
            "training_loss: 2.8110  (2.8905)\ttime: 709s\n",
            "training_loss: 2.7565  (2.8891)\ttime: 709s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.9697  (2.8899)\ttime: 723s\n",
            "training_loss: 2.7285  (2.8883)\ttime: 724s\n",
            "training_loss: 2.8500  (2.8879)\ttime: 724s\n",
            "training_loss: 2.7808  (2.8869)\ttime: 724s\n",
            "training_loss: 2.7208  (2.8852)\ttime: 725s\n",
            "training_loss: 2.6770  (2.8831)\ttime: 725s\n",
            "training_loss: 2.5801  (2.8801)\ttime: 726s\n",
            "training_loss: 2.4461  (2.8757)\ttime: 726s\n",
            "training_loss: 2.5539  (2.8725)\ttime: 726s\n",
            "training_loss: 2.4733  (2.8685)\ttime: 727s\n",
            "training_loss: 2.4939  (2.8648)\ttime: 727s\n",
            "training_loss: 2.5151  (2.8613)\ttime: 728s\n",
            "training_loss: 2.5787  (2.8585)\ttime: 728s\n",
            "training_loss: 2.4044  (2.8539)\ttime: 728s\n",
            "training_loss: 2.3967  (2.8494)\ttime: 729s\n",
            "training_loss: 2.5014  (2.8459)\ttime: 729s\n",
            "training_loss: 2.5569  (2.8430)\ttime: 730s\n",
            "training_loss: 2.7108  (2.8417)\ttime: 730s\n",
            "training_loss: 2.7275  (2.8405)\ttime: 730s\n",
            "training_loss: 2.6050  (2.8382)\ttime: 731s\n",
            "training_loss: 2.5172  (2.8350)\ttime: 731s\n",
            "training_loss: 2.5048  (2.8317)\ttime: 731s\n",
            "training_loss: 2.6774  (2.8301)\ttime: 732s\n",
            "training_loss: 2.7448  (2.8293)\ttime: 732s\n",
            "training_loss: 2.6039  (2.8270)\ttime: 733s\n",
            "training_loss: 2.6293  (2.8250)\ttime: 733s\n",
            "training_loss: 2.5448  (2.8222)\ttime: 733s\n",
            "training_loss: 2.4624  (2.8186)\ttime: 734s\n",
            "training_loss: 2.7665  (2.8181)\ttime: 734s\n",
            "training_loss: 2.8991  (2.8189)\ttime: 735s\n",
            "training_loss: 2.8228  (2.8190)\ttime: 735s\n",
            "training_loss: 2.5668  (2.8164)\ttime: 735s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.8857  (2.8171)\ttime: 750s\n",
            "training_loss: 2.8072  (2.8170)\ttime: 750s\n",
            "training_loss: 2.9440  (2.8183)\ttime: 751s\n",
            "training_loss: 3.0645  (2.8208)\ttime: 751s\n",
            "training_loss: 3.0941  (2.8235)\ttime: 751s\n",
            "training_loss: 2.9715  (2.8250)\ttime: 752s\n",
            "training_loss: 3.0230  (2.8269)\ttime: 752s\n",
            "training_loss: 2.9646  (2.8283)\ttime: 753s\n",
            "training_loss: 2.9652  (2.8297)\ttime: 753s\n",
            "training_loss: 2.7039  (2.8284)\ttime: 753s\n",
            "training_loss: 2.5423  (2.8256)\ttime: 754s\n",
            "training_loss: 2.8573  (2.8259)\ttime: 754s\n",
            "training_loss: 2.9962  (2.8276)\ttime: 755s\n",
            "training_loss: 2.9078  (2.8284)\ttime: 755s\n",
            "training_loss: 2.9059  (2.8292)\ttime: 755s\n",
            "training_loss: 2.8012  (2.8289)\ttime: 756s\n",
            "training_loss: 2.8413  (2.8290)\ttime: 756s\n",
            "training_loss: 2.9614  (2.8303)\ttime: 756s\n",
            "training_loss: 2.8716  (2.8308)\ttime: 757s\n",
            "training_loss: 2.6452  (2.8289)\ttime: 757s\n",
            "training_loss: 2.7628  (2.8282)\ttime: 758s\n",
            "training_loss: 2.9853  (2.8298)\ttime: 758s\n",
            "training_loss: 2.9281  (2.8308)\ttime: 758s\n",
            "training_loss: 2.8777  (2.8313)\ttime: 759s\n",
            "training_loss: 2.8438  (2.8314)\ttime: 759s\n",
            "training_loss: 2.9118  (2.8322)\ttime: 760s\n",
            "training_loss: 2.9878  (2.8337)\ttime: 760s\n",
            "training_loss: 2.9741  (2.8351)\ttime: 760s\n",
            "training_loss: 2.7901  (2.8347)\ttime: 761s\n",
            "training_loss: 2.7191  (2.8335)\ttime: 761s\n",
            "training_loss: 2.9264  (2.8345)\ttime: 762s\n",
            "training_loss: 2.6742  (2.8329)\ttime: 762s\n",
            "/content/samplernn-pytorch/trainer/plugins.py:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input = Variable(input, volatile=True)\n",
            "/content/samplernn-pytorch/trainer/plugins.py:66: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  batch_target = Variable(batch_target, volatile=True)\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "/content/samplernn-pytorch/trainer/plugins.py:72: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  .data[0] * batch_size\n",
            "/content/samplernn-pytorch/trainer/plugins.py:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input = Variable(input, volatile=True)\n",
            "/content/samplernn-pytorch/trainer/plugins.py:66: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  batch_target = Variable(batch_target, volatile=True)\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "/content/samplernn-pytorch/trainer/plugins.py:72: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  .data[0] * batch_size\n",
            "/content/samplernn-pytorch/model.py:267: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  volatile=True\n",
            "/content/samplernn-pytorch/model.py:287: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  volatile=True\n",
            "################################################################################\n",
            "Epoch summary:\n",
            "training_loss: 2.8198\tvalidation_loss: 3.0042\ttest_loss: 2.8433\n",
            "################################################################################\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.8193  (2.8327)\ttime: 861s\n",
            "training_loss: 2.9880  (2.8343)\ttime: 861s\n",
            "training_loss: 3.0445  (2.8364)\ttime: 862s\n",
            "training_loss: 3.1653  (2.8397)\ttime: 862s\n",
            "training_loss: 3.1172  (2.8425)\ttime: 863s\n",
            "training_loss: 3.0610  (2.8446)\ttime: 863s\n",
            "training_loss: 3.0358  (2.8465)\ttime: 863s\n",
            "training_loss: 3.2661  (2.8507)\ttime: 864s\n",
            "training_loss: 2.9957  (2.8522)\ttime: 864s\n",
            "training_loss: 2.9610  (2.8533)\ttime: 864s\n",
            "training_loss: 2.8817  (2.8536)\ttime: 865s\n",
            "training_loss: 2.8136  (2.8532)\ttime: 865s\n",
            "training_loss: 2.7936  (2.8526)\ttime: 866s\n",
            "training_loss: 2.9108  (2.8532)\ttime: 866s\n",
            "training_loss: 3.0382  (2.8550)\ttime: 866s\n",
            "training_loss: 3.0054  (2.8565)\ttime: 867s\n",
            "training_loss: 3.0903  (2.8588)\ttime: 867s\n",
            "training_loss: 2.9013  (2.8593)\ttime: 868s\n",
            "training_loss: 2.8162  (2.8588)\ttime: 868s\n",
            "training_loss: 2.6747  (2.8570)\ttime: 868s\n",
            "training_loss: 2.7268  (2.8557)\ttime: 869s\n",
            "training_loss: 2.6647  (2.8538)\ttime: 869s\n",
            "training_loss: 2.8773  (2.8540)\ttime: 870s\n",
            "training_loss: 3.1141  (2.8566)\ttime: 870s\n",
            "training_loss: 2.9460  (2.8575)\ttime: 870s\n",
            "training_loss: 2.7500  (2.8564)\ttime: 871s\n",
            "training_loss: 2.7096  (2.8550)\ttime: 871s\n",
            "training_loss: 2.6505  (2.8529)\ttime: 872s\n",
            "training_loss: 2.7607  (2.8520)\ttime: 872s\n",
            "training_loss: 2.8805  (2.8523)\ttime: 872s\n",
            "training_loss: 2.9736  (2.8535)\ttime: 873s\n",
            "training_loss: 2.6769  (2.8517)\ttime: 873s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.8248  (2.8515)\ttime: 887s\n",
            "training_loss: 2.7274  (2.8502)\ttime: 888s\n",
            "training_loss: 2.7158  (2.8489)\ttime: 888s\n",
            "training_loss: 2.7750  (2.8481)\ttime: 889s\n",
            "training_loss: 2.6924  (2.8466)\ttime: 889s\n",
            "training_loss: 2.6535  (2.8447)\ttime: 889s\n",
            "training_loss: 2.7942  (2.8442)\ttime: 890s\n",
            "training_loss: 2.7889  (2.8436)\ttime: 890s\n",
            "training_loss: 3.0252  (2.8454)\ttime: 891s\n",
            "training_loss: 2.8472  (2.8454)\ttime: 891s\n",
            "training_loss: 2.6343  (2.8433)\ttime: 891s\n",
            "training_loss: 2.6064  (2.8410)\ttime: 892s\n",
            "training_loss: 2.6676  (2.8392)\ttime: 892s\n",
            "training_loss: 2.7656  (2.8385)\ttime: 893s\n",
            "training_loss: 2.9136  (2.8392)\ttime: 893s\n",
            "training_loss: 2.7070  (2.8379)\ttime: 893s\n",
            "training_loss: 2.5315  (2.8348)\ttime: 894s\n",
            "training_loss: 2.6989  (2.8335)\ttime: 894s\n",
            "training_loss: 2.5696  (2.8308)\ttime: 894s\n",
            "training_loss: 2.3206  (2.8257)\ttime: 895s\n",
            "training_loss: 2.5696  (2.8232)\ttime: 895s\n",
            "training_loss: 2.6753  (2.8217)\ttime: 896s\n",
            "training_loss: 2.6882  (2.8204)\ttime: 896s\n",
            "training_loss: 2.6139  (2.8183)\ttime: 896s\n",
            "training_loss: 2.7076  (2.8172)\ttime: 897s\n",
            "training_loss: 2.7233  (2.8163)\ttime: 897s\n",
            "training_loss: 2.7441  (2.8155)\ttime: 898s\n",
            "training_loss: 2.6392  (2.8138)\ttime: 898s\n",
            "training_loss: 2.5826  (2.8115)\ttime: 898s\n",
            "training_loss: 2.6375  (2.8097)\ttime: 899s\n",
            "training_loss: 2.8369  (2.8100)\ttime: 899s\n",
            "training_loss: 2.6938  (2.8088)\ttime: 899s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.7069  (2.8078)\ttime: 914s\n",
            "training_loss: 2.7442  (2.8072)\ttime: 915s\n",
            "training_loss: 2.7275  (2.8064)\ttime: 915s\n",
            "training_loss: 2.6711  (2.8050)\ttime: 915s\n",
            "training_loss: 2.5417  (2.8024)\ttime: 916s\n",
            "training_loss: 2.6208  (2.8006)\ttime: 916s\n",
            "training_loss: 2.5894  (2.7985)\ttime: 916s\n",
            "training_loss: 2.4893  (2.7954)\ttime: 917s\n",
            "training_loss: 2.5841  (2.7933)\ttime: 917s\n",
            "training_loss: 2.6151  (2.7915)\ttime: 918s\n",
            "training_loss: 2.7356  (2.7909)\ttime: 918s\n",
            "training_loss: 2.8601  (2.7916)\ttime: 918s\n",
            "training_loss: 2.7031  (2.7907)\ttime: 919s\n",
            "training_loss: 2.8019  (2.7908)\ttime: 919s\n",
            "training_loss: 2.7625  (2.7906)\ttime: 920s\n",
            "training_loss: 2.6418  (2.7891)\ttime: 920s\n",
            "training_loss: 2.6323  (2.7875)\ttime: 920s\n",
            "training_loss: 2.6439  (2.7861)\ttime: 921s\n",
            "training_loss: 2.5312  (2.7835)\ttime: 921s\n",
            "training_loss: 2.5305  (2.7810)\ttime: 922s\n",
            "training_loss: 2.6331  (2.7795)\ttime: 922s\n",
            "training_loss: 2.4807  (2.7765)\ttime: 922s\n",
            "training_loss: 2.3966  (2.7727)\ttime: 923s\n",
            "training_loss: 2.5952  (2.7709)\ttime: 923s\n",
            "training_loss: 2.5944  (2.7692)\ttime: 923s\n",
            "training_loss: 2.6699  (2.7682)\ttime: 924s\n",
            "training_loss: 2.6558  (2.7671)\ttime: 924s\n",
            "training_loss: 2.8190  (2.7676)\ttime: 925s\n",
            "training_loss: 2.6732  (2.7666)\ttime: 925s\n",
            "training_loss: 2.5223  (2.7642)\ttime: 925s\n",
            "training_loss: 2.5881  (2.7624)\ttime: 926s\n",
            "training_loss: 2.4721  (2.7595)\ttime: 926s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.7573  (2.7595)\ttime: 941s\n",
            "training_loss: 2.5979  (2.7579)\ttime: 941s\n",
            "training_loss: 2.7376  (2.7577)\ttime: 941s\n",
            "training_loss: 2.7795  (2.7579)\ttime: 942s\n",
            "training_loss: 2.8578  (2.7589)\ttime: 942s\n",
            "training_loss: 2.6038  (2.7574)\ttime: 943s\n",
            "training_loss: 2.5769  (2.7555)\ttime: 943s\n",
            "training_loss: 2.5265  (2.7533)\ttime: 943s\n",
            "training_loss: 2.5195  (2.7509)\ttime: 944s\n",
            "training_loss: 2.3668  (2.7471)\ttime: 944s\n",
            "training_loss: 2.2615  (2.7422)\ttime: 945s\n",
            "training_loss: 2.4118  (2.7389)\ttime: 945s\n",
            "training_loss: 2.3815  (2.7353)\ttime: 945s\n",
            "training_loss: 2.2689  (2.7307)\ttime: 946s\n",
            "training_loss: 2.4234  (2.7276)\ttime: 946s\n",
            "training_loss: 2.7234  (2.7276)\ttime: 947s\n",
            "training_loss: 2.7366  (2.7277)\ttime: 947s\n",
            "training_loss: 2.5307  (2.7257)\ttime: 947s\n",
            "training_loss: 2.4494  (2.7229)\ttime: 948s\n",
            "training_loss: 2.6584  (2.7223)\ttime: 948s\n",
            "training_loss: 2.6656  (2.7217)\ttime: 948s\n",
            "training_loss: 2.6788  (2.7213)\ttime: 949s\n",
            "training_loss: 2.6104  (2.7202)\ttime: 949s\n",
            "training_loss: 2.5481  (2.7185)\ttime: 950s\n",
            "training_loss: 2.5143  (2.7164)\ttime: 950s\n",
            "training_loss: 2.4714  (2.7140)\ttime: 950s\n",
            "training_loss: 2.5421  (2.7122)\ttime: 951s\n",
            "training_loss: 2.4939  (2.7101)\ttime: 951s\n",
            "training_loss: 2.6547  (2.7095)\ttime: 952s\n",
            "training_loss: 2.7462  (2.7099)\ttime: 952s\n",
            "training_loss: 2.8429  (2.7112)\ttime: 952s\n",
            "training_loss: 2.7878  (2.7120)\ttime: 953s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.8327  (2.7132)\ttime: 967s\n",
            "training_loss: 2.7268  (2.7133)\ttime: 968s\n",
            "training_loss: 2.6284  (2.7125)\ttime: 968s\n",
            "training_loss: 2.6920  (2.7123)\ttime: 968s\n",
            "training_loss: 2.9251  (2.7144)\ttime: 969s\n",
            "training_loss: 2.8756  (2.7160)\ttime: 969s\n",
            "training_loss: 2.7379  (2.7162)\ttime: 969s\n",
            "training_loss: 2.7438  (2.7165)\ttime: 970s\n",
            "training_loss: 2.7431  (2.7168)\ttime: 970s\n",
            "training_loss: 2.6533  (2.7161)\ttime: 971s\n",
            "training_loss: 2.6668  (2.7156)\ttime: 971s\n",
            "training_loss: 2.7551  (2.7160)\ttime: 971s\n",
            "training_loss: 2.7182  (2.7161)\ttime: 972s\n",
            "training_loss: 2.8942  (2.7178)\ttime: 972s\n",
            "training_loss: 2.8280  (2.7189)\ttime: 973s\n",
            "training_loss: 2.7152  (2.7189)\ttime: 973s\n",
            "training_loss: 2.6522  (2.7182)\ttime: 973s\n",
            "training_loss: 2.7647  (2.7187)\ttime: 974s\n",
            "training_loss: 2.6397  (2.7179)\ttime: 974s\n",
            "training_loss: 2.5505  (2.7162)\ttime: 975s\n",
            "training_loss: 2.7501  (2.7166)\ttime: 975s\n",
            "training_loss: 2.6807  (2.7162)\ttime: 975s\n",
            "training_loss: 2.4791  (2.7138)\ttime: 976s\n",
            "training_loss: 2.5324  (2.7120)\ttime: 976s\n",
            "training_loss: 2.5737  (2.7106)\ttime: 976s\n",
            "training_loss: 2.5731  (2.7093)\ttime: 977s\n",
            "training_loss: 2.7074  (2.7092)\ttime: 977s\n",
            "training_loss: 2.7762  (2.7099)\ttime: 978s\n",
            "training_loss: 3.1055  (2.7139)\ttime: 978s\n",
            "training_loss: 3.1570  (2.7183)\ttime: 978s\n",
            "training_loss: 2.8504  (2.7196)\ttime: 979s\n",
            "training_loss: 2.4922  (2.7174)\ttime: 979s\n",
            "/content/samplernn-pytorch/trainer/plugins.py:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input = Variable(input, volatile=True)\n",
            "/content/samplernn-pytorch/trainer/plugins.py:66: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  batch_target = Variable(batch_target, volatile=True)\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "/content/samplernn-pytorch/trainer/plugins.py:72: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  .data[0] * batch_size\n",
            "/content/samplernn-pytorch/trainer/plugins.py:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input = Variable(input, volatile=True)\n",
            "/content/samplernn-pytorch/trainer/plugins.py:66: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  batch_target = Variable(batch_target, volatile=True)\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "/content/samplernn-pytorch/trainer/plugins.py:72: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  .data[0] * batch_size\n",
            "/content/samplernn-pytorch/model.py:267: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  volatile=True\n",
            "/content/samplernn-pytorch/model.py:287: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  volatile=True\n",
            "################################################################################\n",
            "Epoch summary:\n",
            "training_loss: 2.7116\tvalidation_loss: 2.9587\ttest_loss: 2.7884\n",
            "################################################################################\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.5975  (2.7162)\ttime: 1079s\n",
            "training_loss: 2.5041  (2.7140)\ttime: 1079s\n",
            "training_loss: 2.5976  (2.7129)\ttime: 1080s\n",
            "training_loss: 2.7102  (2.7128)\ttime: 1080s\n",
            "training_loss: 2.4351  (2.7101)\ttime: 1081s\n",
            "training_loss: 2.4545  (2.7075)\ttime: 1081s\n",
            "training_loss: 2.4847  (2.7053)\ttime: 1081s\n",
            "training_loss: 2.3778  (2.7020)\ttime: 1082s\n",
            "training_loss: 2.3992  (2.6990)\ttime: 1082s\n",
            "training_loss: 2.4403  (2.6964)\ttime: 1083s\n",
            "training_loss: 2.4466  (2.6939)\ttime: 1083s\n",
            "training_loss: 2.2510  (2.6895)\ttime: 1083s\n",
            "training_loss: 2.1942  (2.6845)\ttime: 1084s\n",
            "training_loss: 2.2299  (2.6800)\ttime: 1084s\n",
            "training_loss: 2.5096  (2.6783)\ttime: 1084s\n",
            "training_loss: 2.4658  (2.6761)\ttime: 1085s\n",
            "training_loss: 2.4649  (2.6740)\ttime: 1085s\n",
            "training_loss: 2.4394  (2.6717)\ttime: 1086s\n",
            "training_loss: 2.4522  (2.6695)\ttime: 1086s\n",
            "training_loss: 2.3230  (2.6660)\ttime: 1086s\n",
            "training_loss: 2.4652  (2.6640)\ttime: 1087s\n",
            "training_loss: 2.4700  (2.6621)\ttime: 1087s\n",
            "training_loss: 2.4653  (2.6601)\ttime: 1088s\n",
            "training_loss: 2.5712  (2.6592)\ttime: 1088s\n",
            "training_loss: 2.5582  (2.6582)\ttime: 1088s\n",
            "training_loss: 2.6288  (2.6579)\ttime: 1089s\n",
            "training_loss: 2.6333  (2.6577)\ttime: 1089s\n",
            "training_loss: 2.7330  (2.6584)\ttime: 1090s\n",
            "training_loss: 2.9038  (2.6609)\ttime: 1090s\n",
            "training_loss: 2.8258  (2.6625)\ttime: 1090s\n",
            "training_loss: 2.8865  (2.6648)\ttime: 1091s\n",
            "training_loss: 2.8070  (2.6662)\ttime: 1091s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.8871  (2.6684)\ttime: 1105s\n",
            "training_loss: 2.9653  (2.6714)\ttime: 1106s\n",
            "training_loss: 2.8985  (2.6736)\ttime: 1106s\n",
            "training_loss: 2.7700  (2.6746)\ttime: 1107s\n",
            "training_loss: 2.9608  (2.6775)\ttime: 1107s\n",
            "training_loss: 2.8616  (2.6793)\ttime: 1107s\n",
            "training_loss: 2.7891  (2.6804)\ttime: 1108s\n",
            "training_loss: 2.8799  (2.6824)\ttime: 1108s\n",
            "training_loss: 2.7159  (2.6827)\ttime: 1108s\n",
            "training_loss: 2.6421  (2.6823)\ttime: 1109s\n",
            "training_loss: 2.5621  (2.6811)\ttime: 1109s\n",
            "training_loss: 2.7138  (2.6814)\ttime: 1110s\n",
            "training_loss: 2.6381  (2.6810)\ttime: 1110s\n",
            "training_loss: 2.6820  (2.6810)\ttime: 1110s\n",
            "training_loss: 2.9227  (2.6834)\ttime: 1111s\n",
            "training_loss: 2.8924  (2.6855)\ttime: 1111s\n",
            "training_loss: 2.8460  (2.6871)\ttime: 1112s\n",
            "training_loss: 2.8572  (2.6888)\ttime: 1112s\n",
            "training_loss: 2.6076  (2.6880)\ttime: 1112s\n",
            "training_loss: 2.6425  (2.6876)\ttime: 1113s\n",
            "training_loss: 2.7817  (2.6885)\ttime: 1113s\n",
            "training_loss: 2.8321  (2.6899)\ttime: 1114s\n",
            "training_loss: 2.7875  (2.6909)\ttime: 1114s\n",
            "training_loss: 2.9176  (2.6932)\ttime: 1114s\n",
            "training_loss: 2.8094  (2.6943)\ttime: 1115s\n",
            "training_loss: 2.7025  (2.6944)\ttime: 1115s\n",
            "training_loss: 2.8053  (2.6955)\ttime: 1115s\n",
            "training_loss: 2.7885  (2.6965)\ttime: 1116s\n",
            "training_loss: 2.7602  (2.6971)\ttime: 1116s\n",
            "training_loss: 2.8284  (2.6984)\ttime: 1117s\n",
            "training_loss: 2.8113  (2.6995)\ttime: 1117s\n",
            "training_loss: 2.5313  (2.6979)\ttime: 1117s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 3.0182  (2.7011)\ttime: 1132s\n",
            "training_loss: 2.9093  (2.7031)\ttime: 1132s\n",
            "training_loss: 2.9605  (2.7057)\ttime: 1133s\n",
            "training_loss: 3.0587  (2.7093)\ttime: 1133s\n",
            "training_loss: 2.9197  (2.7114)\ttime: 1133s\n",
            "training_loss: 2.7086  (2.7113)\ttime: 1134s\n",
            "training_loss: 2.8023  (2.7122)\ttime: 1134s\n",
            "training_loss: 2.9006  (2.7141)\ttime: 1134s\n",
            "training_loss: 2.7816  (2.7148)\ttime: 1135s\n",
            "training_loss: 2.7799  (2.7154)\ttime: 1135s\n",
            "training_loss: 2.7593  (2.7159)\ttime: 1136s\n",
            "training_loss: 2.8876  (2.7176)\ttime: 1136s\n",
            "training_loss: 3.1474  (2.7219)\ttime: 1136s\n",
            "training_loss: 3.1564  (2.7262)\ttime: 1137s\n",
            "training_loss: 2.8670  (2.7277)\ttime: 1137s\n",
            "training_loss: 2.8892  (2.7293)\ttime: 1138s\n",
            "training_loss: 2.8200  (2.7302)\ttime: 1138s\n",
            "training_loss: 2.7294  (2.7302)\ttime: 1138s\n",
            "training_loss: 2.6217  (2.7291)\ttime: 1139s\n",
            "training_loss: 2.3943  (2.7257)\ttime: 1139s\n",
            "training_loss: 2.3920  (2.7224)\ttime: 1140s\n",
            "training_loss: 2.4255  (2.7194)\ttime: 1140s\n",
            "training_loss: 2.4481  (2.7167)\ttime: 1140s\n",
            "training_loss: 2.4069  (2.7136)\ttime: 1141s\n",
            "training_loss: 2.4722  (2.7112)\ttime: 1141s\n",
            "training_loss: 2.3977  (2.7081)\ttime: 1141s\n",
            "training_loss: 2.3201  (2.7042)\ttime: 1142s\n",
            "training_loss: 2.2548  (2.6997)\ttime: 1142s\n",
            "training_loss: 2.3430  (2.6961)\ttime: 1143s\n",
            "training_loss: 2.5425  (2.6946)\ttime: 1143s\n",
            "training_loss: 2.6903  (2.6945)\ttime: 1143s\n",
            "training_loss: 2.5538  (2.6931)\ttime: 1144s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.5433  (2.6916)\ttime: 1158s\n",
            "training_loss: 2.4888  (2.6896)\ttime: 1159s\n",
            "training_loss: 2.5326  (2.6880)\ttime: 1159s\n",
            "training_loss: 2.5961  (2.6871)\ttime: 1159s\n",
            "training_loss: 2.7373  (2.6876)\ttime: 1160s\n",
            "training_loss: 2.8700  (2.6895)\ttime: 1160s\n",
            "training_loss: 2.9397  (2.6920)\ttime: 1161s\n",
            "training_loss: 2.6840  (2.6919)\ttime: 1161s\n",
            "training_loss: 2.5870  (2.6908)\ttime: 1161s\n",
            "training_loss: 2.6316  (2.6902)\ttime: 1162s\n",
            "training_loss: 2.8296  (2.6916)\ttime: 1162s\n",
            "training_loss: 2.8371  (2.6931)\ttime: 1162s\n",
            "training_loss: 2.7732  (2.6939)\ttime: 1163s\n",
            "training_loss: 2.8553  (2.6955)\ttime: 1163s\n",
            "training_loss: 2.9700  (2.6982)\ttime: 1164s\n",
            "training_loss: 2.7703  (2.6990)\ttime: 1164s\n",
            "training_loss: 2.6838  (2.6988)\ttime: 1164s\n",
            "training_loss: 2.6870  (2.6987)\ttime: 1165s\n",
            "training_loss: 2.7494  (2.6992)\ttime: 1165s\n",
            "training_loss: 2.6466  (2.6987)\ttime: 1166s\n",
            "training_loss: 2.8492  (2.7002)\ttime: 1166s\n",
            "training_loss: 2.7523  (2.7007)\ttime: 1166s\n",
            "training_loss: 2.5801  (2.6995)\ttime: 1167s\n",
            "training_loss: 2.5622  (2.6981)\ttime: 1167s\n",
            "training_loss: 2.5721  (2.6969)\ttime: 1168s\n",
            "training_loss: 2.5636  (2.6955)\ttime: 1168s\n",
            "training_loss: 2.6834  (2.6954)\ttime: 1168s\n",
            "training_loss: 2.7420  (2.6959)\ttime: 1169s\n",
            "training_loss: 2.6307  (2.6952)\ttime: 1169s\n",
            "training_loss: 2.5408  (2.6937)\ttime: 1169s\n",
            "training_loss: 2.6245  (2.6930)\ttime: 1170s\n",
            "training_loss: 2.4165  (2.6902)\ttime: 1170s\n",
            "/content/samplernn-pytorch/model.py:181: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x.view(-1, self.q_levels)) \\\n",
            "training_loss: 2.6585  (2.6899)\ttime: 1184s\n",
            "training_loss: 2.7149  (2.6902)\ttime: 1185s\n",
            "training_loss: 2.8050  (2.6913)\ttime: 1185s\n",
            "training_loss: 2.7422  (2.6918)\ttime: 1186s\n",
            "training_loss: 2.8104  (2.6930)\ttime: 1186s\n",
            "training_loss: 2.7399  (2.6935)\ttime: 1186s\n",
            "training_loss: 2.6552  (2.6931)\ttime: 1187s\n",
            "training_loss: 2.6141  (2.6923)\ttime: 1187s\n",
            "training_loss: 2.8693  (2.6941)\ttime: 1188s\n",
            "training_loss: 2.5493  (2.6926)\ttime: 1188s\n",
            "training_loss: 2.3722  (2.6894)\ttime: 1188s\n",
            "training_loss: 2.5401  (2.6879)\ttime: 1189s\n",
            "training_loss: 2.4006  (2.6850)\ttime: 1189s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs6xEHVEzmDm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}